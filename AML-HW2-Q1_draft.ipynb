{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------\n",
    "# Implementing a Neural Network In this exercise we will develop a neural\n",
    "# network with fully-connected layers to perform classification, and test it\n",
    "# out on the CIFAR-10 dataset.  A bit of setup\n",
    "#-----------------------------------------------------------------------------------\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "\n",
    "#from two_layernet import TwoLayerNet\n",
    "#from gradient_check import eval_numerical_gradient\n",
    "#from data_utils import get_CIFAR10_data\n",
    "#from vis_utils import visualize_grid\n",
    "#-------------------------- * End of setup *-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "# Some helper functions\n",
    "# ------------------------------------------------------\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#-------------------------- * End of helper functions *--------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([[3.0, 1.0, 2.0],[5.0,3.0,9.0]]) # just testing,for better understanding of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  30.19287485, 8271.5826236 ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((np.exp(scores)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  30.19287485],\n",
       "       [8271.5826236 ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((np.exp(scores)),axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66524096, 0.09003057, 0.24472847],\n",
       "       [0.01794253, 0.00242826, 0.97962921]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(scores) / np.sum((np.exp(scores)),axis=1,keepdims=True) # this is essentialy the softmax, each row sums to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of\n",
    "    N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "    connected layer.\n",
    "\n",
    "    In other words, the network has the following architecture:\n",
    "\n",
    "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two-layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2'] #shapes 10,3 -- 3\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = 0.\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: Perform the forward pass, computing the class probabilities for the #\n",
    "        # input. Store the result in the scores variable, which should be an array  #\n",
    "        # of shape (N, C).                                                          #\n",
    "        #############################################################################\n",
    "        \n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        first_layer=np.matmul(X,W1)+b1\n",
    "        \n",
    "        first_layer_activation=np.clip(a=first_layer,a_min=0,a_max=None)\n",
    "        \n",
    "        second_layer=np.matmul(first_layer_activation,W2)+b2\n",
    "        \n",
    "        scores=np.exp(second_layer) / np.sum((np.exp(second_layer)),axis=1,keepdims=True)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = 0.\n",
    "        #############################################################################\n",
    "        # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "        # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "        # in the variable loss, which should be a scalar. Use the Softmax           #\n",
    "        # classifier loss.                                                          #\n",
    "        #############################################################################\n",
    "        \n",
    "        # Implement the loss for the softmax output layer\n",
    "        \n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        y_onehot = np.zeros((y.size, y.max()+1))\n",
    "        y_onehot[np.arange(y.size),y] = 1 # transform to one-hot so we can multiply,and take just the log of the probability. \n",
    "                                            #of the correct value,since all the other values will be zero,for each instance\n",
    "        \n",
    "        data_loss=-np.sum(y_onehot*np.log(scores))/y.size\n",
    "        regularization_loss=reg*((np.linalg.norm(W1)**2)+(np.linalg.norm(W2)**2)) # default value of np.linalg.norm is Euclidean-Frobenius-l2 norm\n",
    "        \n",
    "        loss=data_loss+regularization_loss\n",
    "                \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}\n",
    "\n",
    "        ##############################################################################\n",
    "        # TODO: Implement the backward pass, computing the derivatives of the weights#\n",
    "        # and biases. Store the results in the grads dictionary. For example,        #\n",
    "        # grads['W1'] should store the gradient on W1, and be a matrix of same size  #\n",
    "        ##############################################################################\n",
    "\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        \n",
    "\n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        return loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[0.3644621  0.22911264 0.40642526]\n",
      " [0.47590629 0.17217039 0.35192332]\n",
      " [0.43035767 0.26164229 0.30800004]\n",
      " [0.41583127 0.2983228  0.28584593]\n",
      " [0.36328815 0.32279939 0.31391246]]\n",
      "\n",
      "correct scores:\n",
      "[[0.3644621  0.22911264 0.40642526]\n",
      " [0.47590629 0.17217039 0.35192332]\n",
      " [0.43035767 0.26164229 0.30800004]\n",
      " [0.41583127 0.2983228  0.28584593]\n",
      " [0.36328815 0.32279939 0.31391246]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    " [0.36446210, 0.22911264, 0.40642526],\n",
    " [0.47590629, 0.17217039, 0.35192332],\n",
    " [0.43035767, 0.26164229, 0.30800004],\n",
    " [0.41583127, 0.29832280, 0.28584593],\n",
    " [0.36328815, 0.32279939, 0.31391246]])\n",
    "print(correct_scores)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your scores and correct scores:\n",
      "2.917341163088949e-08\n"
     ]
    }
   ],
   "source": [
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_onehot = np.zeros((y.size, y.max()+1))\n",
    "#y_onehot[np.arange(y.size),y] = 1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3644621 , 0.        , 0.        ],\n",
       "       [0.        , 0.17217039, 0.        ],\n",
       "       [0.        , 0.        , 0.30800004],\n",
       "       [0.        , 0.        , 0.28584593],\n",
       "       [0.        , 0.32279939, 0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_onehot*scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3644621 , 0.22911264, 0.40642526],\n",
       "       [0.47590629, 0.17217039, 0.35192332],\n",
       "       [0.43035767, 0.26164229, 0.30800004],\n",
       "       [0.41583127, 0.2983228 , 0.28584593],\n",
       "       [0.36328815, 0.32279939, 0.31391246]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "1.794120407794253e-13\n"
     ]
    }
   ],
   "source": [
    "# Forward pass: compute loss. In the same function, implement the second part\n",
    "# that computes the data and regularization loss.\n",
    "loss,_ = net.loss(X, y, reg=0.05) # remove the comma and the _ if you want to get the loss,right now the function returns just 1 element\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
